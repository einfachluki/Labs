{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae91306f",
   "metadata": {},
   "source": [
    "# Presentation Structure\n",
    "\n",
    "## Data Analytics Lifecycle\n",
    "### 1. Define\n",
    "- Pre-done\n",
    "\n",
    "### 2. Gather Data\n",
    "- Pre-done\n",
    "\n",
    "### 3. Cleaning\n",
    "1. Standardize Columns to aggregate (sort & lowercase & concatenate files)\n",
    "2. ST Column -> rename to state column\n",
    "3. GENDER Column -> rename to gender column\n",
    "4. Gender Column -> Standardization column to only have \"Female\" and \"Male\"\n",
    "5. State column -> Add extra column to show \"West\", \"East\", \"North\", \"South\" regions\n",
    "6. Transform columns into their intended data type (e.g. remove \"%\" and extract 0/0/00)\n",
    "7. Drop duplicated rows\n",
    "8. Drop data values that show outliers with the help of IQR method\n",
    "\n",
    "### 4. Exploring\n",
    "- Keep business objectives in mind:\n",
    "    1. Retain customers\n",
    "    2. Analyze relevant customer data\n",
    "    3. Develop focused customer retention programs\n",
    "- Customer Lifetime Value Analysis: Column Monthly_premium_auot <-> Column CLF show highest correlation\n",
    "- Region Analysis\n",
    "- Education Analysis\n",
    "\n",
    "### 5. Processing\n",
    "- upcoming in future weeks\n",
    "\n",
    "### 6. Apply Model\n",
    "- upcoming in future weeks\n",
    "\n",
    "### 7. Validate\n",
    "- upcoming in future weeks\n",
    "\n",
    "### 8. Present\n",
    "- upcoming in future weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632e520",
   "metadata": {},
   "source": [
    "## Weeky 2 - Monday: Activity 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c0c64",
   "metadata": {},
   "source": [
    "### Task 1: Aggregate the data into one Data Frame using Pandas. Pay attention that files may have different names for the same column. therefore, make sure that you unify the columns names before concating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866292a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing neccessary Python libraries (e.g. Pandas, Numpy, Matplotlib, Seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd365215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing file1 and previewing setup\n",
    "#Anomalies: column \"ST\" and \"GENDER\"\n",
    "file1 = pd.read_csv(\"file1.csv\")\n",
    "file1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing file1 and previewing setup\n",
    "#Anomalies: column \"ST\" and \"GENDER\"\n",
    "file2 = pd.read_csv(\"file2.csv\")\n",
    "file2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing file1 and previewing setup\n",
    "#Anomalies: No anomalies found in column's names\n",
    "file3 = pd.read_csv(\"file3.csv\")\n",
    "file3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93eed1b",
   "metadata": {},
   "source": [
    "### Task 2: Standardizing header names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77dbcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to standardize file1, file2, file3 headers names and hence fix found anomalies \n",
    "#Function to lower case the column names -> allows for sorting them and hence aggregating them more easily\n",
    "def lower_case_column_names(dataframe):\n",
    "    dataframe.columns=[i.lower() for i in dataframe.columns]\n",
    "    return dataframe\n",
    "\n",
    "#Function to clean \"st\" column in file1 and file2\n",
    "def clean_state_column(dataframe):\n",
    "    dataframe.rename(columns={'st':'state'}, inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying above created functions to allow for concatenating file1, file2, file3\n",
    "lower_case_column_names(file1)\n",
    "lower_case_column_names(file2)\n",
    "lower_case_column_names(file3)\n",
    "clean_state_column(file1)\n",
    "clean_state_column(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ac3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting dataframes to hence allow for concatenating\n",
    "file1 = file1.sort_index(axis=1)\n",
    "file2 = file2.sort_index(axis=1)\n",
    "file3 = file3.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double check if all columns are equal. If yes -> ready for concatenating!\n",
    "file1.columns == file2.columns\n",
    "file2.columns == file3.columns\n",
    "file1.columns == file3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07179832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the data into one new dataframe, called \"df\"\n",
    "df = pd.concat([file1,file2,file3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering some basic information about concatenated dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3372e3",
   "metadata": {},
   "source": [
    "### Task 3: Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71855dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7f202",
   "metadata": {},
   "source": [
    "### Task 4: Deleting and rearranging columns – delete the column customer as it is only a unique identifier for each row of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting and rearranging the column \"customer\"\n",
    "df = df.drop(\"customer\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded461bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the index after recently deleted \"customer\" column\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df6b4a",
   "metadata": {},
   "source": [
    "### Task 5: Working with data types – Check the data types of all the columns and fix the incorrect ones (for ex. customer lifetime value and number of open complaints ). Hint: remove the percentage from the customer lifetime value and truncate it to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82657b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering information about each of the df's columns and their data type\n",
    "#Anomalies: \n",
    "    #1) \"customer lifetime value\"-column is of type string/object whereas the preview states soly integers => requires analysis\n",
    "    #2) \"number of open complaints\"-column is of type string/object whereas the preview states integers => requires analysis\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0807f9",
   "metadata": {},
   "source": [
    "### Task 5.1: Customer Lifetime Value Column Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply lamda function to delete \"&\" signs of \"customer lifetime value\" column\n",
    "df[\"customer lifetime value\"] = df[\"customer lifetime value\"].apply(lambda x: str(x).replace('%', ''))\n",
    "df[\"customer lifetime value\"] =  pd.to_numeric(df[\"customer lifetime value\"], errors='coerce')\n",
    "\n",
    "#Double check if datatype of column has changed from object to numeric (int or float). Otherwise, further analysis required\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double check, how many NaN values are still present in the \"customer lifetime value\"-column\n",
    "#Food for thought: might delete NaN in later stage of EDA if necessary\n",
    "df[df[\"customer lifetime value\"].isnull() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400a4bd",
   "metadata": {},
   "source": [
    "### Task 5.2: Number of Open Complaints Analysis / clean the number of open complaints and extract the middle number which is changing between records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate all values (str/int/float etc) of \"Number of Open Complaints\"-column into numeric data column, and ignore any upcoming error\n",
    "df[\"number of open complaints\"] =  pd.to_numeric(df[\"number of open complaints\"], errors='ignore')\n",
    "\n",
    "#Double check, which values have not been transformed into numeric values and count these\n",
    "df[df[\"number of open complaints\"].str.isnumeric() == False]\n",
    "df[df[\"number of open complaints\"].str.isnumeric() == False][\"number of open complaints\"].value_counts()\n",
    "\n",
    "#Next steps: find method with which you can extract middle part of x/x/xx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa573c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identified concept on how to pull second value between / and / and transform value into int (in principle)\n",
    "df[\"number of open complaints\"][0].split(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply above identified concept on all entries in the \"number of open complaints\"-column, using the apply function\n",
    "df[\"number of open complaints\"] = df[\"number of open complaints\"].apply(lambda x: int(x.split('/')[1]) if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double check if now all values of \"number of open complaints\"-column are of type int\n",
    "df[\"number of open complaints\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd9582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Double check if now all values of \"number of open complaints\"-column are of type int\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abece4",
   "metadata": {},
   "source": [
    "### Task 6: Filtering data and Correcting typos – Filter the data in state and gender column to standardize the texts in those column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a8707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First identifying the unique options in the \"gender\"-column\n",
    "df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First identifying the unique options in the \"state\"-column\n",
    "df[\"state\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70d1be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function, that standardizes the column \"gender\" based on the in the previous step identified unique categories\n",
    "def standardize_gender(x):\n",
    "    if x in [\"F\", \"female\", \"Femal\"]:\n",
    "        return \"Female\"\n",
    "    elif x in [\"M\", \"Male\"]:\n",
    "        return \"Male\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function, that standardizes the column \"state\" based on the in the previous step identified unique categories\n",
    "def standardize_state(x):\n",
    "    if x in [\"California\", \"Cali\"]:\n",
    "        return \"California\"\n",
    "    elif x in [\"Arizona\", \"AZ\"]:\n",
    "        return \"Arizona\"\n",
    "    elif x in [\"Washington\", \"WA\"]:\n",
    "        return \"Washington\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d113d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the previously built function\n",
    "df[\"gender\"] = df[\"gender\"].apply(standardize_gender)\n",
    "df[\"state\"] = df[\"state\"].apply(standardize_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecda661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double checking if applied function works as intended\n",
    "df[\"state\"].value_counts()\n",
    "df[\"state\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the final dataset to double check for any other anomanlies\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d6139",
   "metadata": {},
   "source": [
    "## Weeky 2 - Tuesday: Activity 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replacing null values – Replace missing values with means of the column (for numerical columns). Pay attention that the Income feature for instance has 0s which is equivalent to null values. (We assume here that there is no such income with 0 as it refers to missing values) Hint: numpy.nan is considered of float64 data type.\n",
    "##Bucketing the data - Write a function to replace column \"State\" to different zones. California as West Region, Oregon as North West, and Washington as East, and Arizona and Nevada as Central\n",
    "##(Optional) In the column Vehicle Class, merge the two categories Luxury SUV and Luxury Car into one category named Luxury Vehicle\n",
    "##(Optional) Removing outliers using 1.5*IQR technique for all numerical columns.\n",
    "##(Optional) Standardizing the data – Use string functions to standardize the text data (lower case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d782b5",
   "metadata": {},
   "source": [
    "### Task 1: Replacing null values – Replace missing values with means of the column (for numerical columns). Pay attention that the Income feature for instance has 0s which is equivalent to null values. (We assume here that there is no such income with 0 as it refers to missing values) Hint: numpy.nan is considered of float64 data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d90029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace NaN values in numerical functions\n",
    "def replace_with_mean(dataframe, clmn):\n",
    "    \n",
    "    if dataframe[clmn].dtype != object:\n",
    "        mean_value = dataframe[clmn].mean()\n",
    "        dataframe[clmn].fillna(value = mean_value, inplace = True)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply previously built function to all numeric columns\n",
    "#Double check if in all numeric columns the NaN values have been removed (via checking if the len(isna()) is 0.\n",
    "for column in df.columns:\n",
    "    replace_with_mean(df,column)\n",
    "    if len(df[column].isna().value_counts()) <= 1:\n",
    "        print(f\"The {column} has been successfully transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec4f15",
   "metadata": {},
   "source": [
    "### Task 2: Bucketing the data - Write a function to replace column \"State\" to different zones. California as West Region, Oregon as North West, and Washington as East, and Arizona and Nevada as Central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build translator dictionary and consequently replace those in the dataframe\n",
    "translator = {\"California\": \"West Region\", \n",
    "                \"Oregon\": \"North West Region\", \n",
    "                \"Washington\": \"East Region\", \n",
    "                \"Nevada\": \"Central\",\n",
    "                \"Arizona\": \"Central\"}\n",
    "\n",
    "df[\"state\"] = df[\"state\"].replace(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed736c",
   "metadata": {},
   "source": [
    "### Task 3: (Optional) In the column Vehicle Class, merge the two categories Luxury SUV and Luxury Car into one category named Luxury Vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build translator dictionary and consequently replace those in the dataframe\n",
    "translator_dict = {\"Luxury SUV\": \"Luxury Vehicle\",\n",
    "            \"Luxury Car\": \"Luxury Vehicle\"}\n",
    "\n",
    "df[\"vehicle class\"] = df[\"vehicle class\"].replace(translator_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3151c2f",
   "metadata": {},
   "source": [
    "### Task 4: (Optional) Removing outliers using 1.5*IQR technique for all numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proof shape of df before outlier removal\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that defines upper and lower limits based on IQR method and consequently replaces lower / larger values with NaN\n",
    "#Function does NOT drop the whole row when there is a NaN found in one column, but instead only replaces the value\n",
    "def remove_outlier(column_name, df):\n",
    "    percentile25 = df[column_name].quantile(0.25)\n",
    "    percentile75 = df[column_name].quantile(0.75)\n",
    "    iqr = percentile75 - percentile25\n",
    "    upper_limit = percentile75 + 1.5 * iqr\n",
    "    lower_limit = percentile25 - 1.5 * iqr\n",
    "    \n",
    "    df_droppers_upper = df[df[column_name] > upper_limit].index\n",
    "    df_droppers_lower = df[df[column_name] < lower_limit].index\n",
    "    total_to_drop = df_droppers_upper.union(df_droppers_lower)\n",
    "    \n",
    "    df = df.drop(total_to_drop, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e04844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function to all columns in df that are of numerical datatype\n",
    "for column in df.columns:\n",
    "    if df[column].dtype != object:\n",
    "        remove_outlier(column, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879aa279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double check if outlier removal has been successfull: shape previous (9135 rows) vs shape after (5221 rows)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index\n",
    "df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0ebe5",
   "metadata": {},
   "source": [
    "### Task 5: (Optional) Standardizing the data – Use string functions to standardize the text data (lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that lower cases the entries in each column that is of type object\n",
    "def standardize(df, clmn):\n",
    "    if df[clmn].dtype == object:\n",
    "        df[clmn] = df[clmn].str.lower()\n",
    "    else:\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63309e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the function to each column in the dataframe\n",
    "for column in df.columns:\n",
    "    standardize(df,column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e223b90c",
   "metadata": {},
   "source": [
    "## Weeky 2 - Wednesday: Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58604c16",
   "metadata": {},
   "source": [
    "### Task 1: Get the numeric data into dataframe called numerical and categorical columns in a dataframe called categoricals. (You can use np.number and np.object to select the numerical data types and categorical data types respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755cbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previewing original data\n",
    "new_df = pd.read_csv(\"/Users/lukasbauerschmidt/Desktop/Ironhack/2. Class Materials/IH_DA_FT_JAN_2023/Class_Materials/Case_Studies/Customer_Analysis_Case_Study/Data/Data_Marketing_Customer_Analysis_Round3.csv\")\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653355e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into numerical only features, allowing for targeted processing and EDA\n",
    "num_df = new_df.select_dtypes(include=['int', 'float'])\n",
    "num_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into caterogical only features, allowing for targeted processing and EDA\n",
    "cat_df = new_df.select_dtypes(exclude=['int', 'float'])\n",
    "cat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7d893",
   "metadata": {},
   "source": [
    "### Task 2: Now we will try to check the normality of the numerical variables visually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c5a4a",
   "metadata": {},
   "source": [
    "### Task 2.1: Use seaborn library to construct distribution plots for the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram with seaborn for each column in the numerical dataframe with for loop\n",
    "fig=plt.figure(figsize=(20,15))\n",
    "for i, column in enumerate(num_df.columns, 1):\n",
    "    plt.subplot(3,3,i)\n",
    "    sns.histplot(num_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646cb4d",
   "metadata": {},
   "source": [
    "### Task 2.2.: Use Matplotlib to construct histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram with matplotlib for each column in the numerical dataframe with for loop\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "for i, column in enumerate(num_df.columns, 1):\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.hist(num_df[column], edgecolor = \"black\")\n",
    "    plt.title(column)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac15a1f",
   "metadata": {},
   "source": [
    "### Task 2.3: Do the distributions for different numerical variables look like a normal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ccec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customer lifetime value & total amount claimed seem to be variables which show right skewed data distribution\n",
    "#Months since policy inception and months since last claim seem to be also close to normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47d6ea",
   "metadata": {},
   "source": [
    "### Task 3: For the numerical variables, check for correlation between the input features. Note: this does not include the target feature. Plot the Correlation Heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the target variable from the numerical dataframe\n",
    "num_df_wo_target = num_df.drop(\"total_claim_amount\", axis=1)\n",
    "\n",
    "#Built correlation matrix of numerical dataframe (w/o target variable)\n",
    "corr = num_df_wo_target.corr()\n",
    "\n",
    "#Plot and layout heatmap using seaborn\n",
    "plt.figure(figsize=(10, 4))\n",
    "heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e7ddf",
   "metadata": {},
   "source": [
    "### Task 4: Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cafe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(new_df, hue=\"region\", height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443e719",
   "metadata": {},
   "source": [
    "## Weeky 2 - Thursday: Activity 4¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d6f64",
   "metadata": {},
   "source": [
    "### Task 1: Show a plot of the total number of responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa3bd5",
   "metadata": {},
   "source": [
    "### Task 2: Show a plot of the response by the sales channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b1ca9",
   "metadata": {},
   "source": [
    "### Task 3: Show a plot of the response by the total claim amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43598a14",
   "metadata": {},
   "source": [
    "### Task 4: Show a plot of the response by income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating extra column in dataframe, that translates responses from boolean to numerical values\n",
    "translator = {\"no\": 0, \"yes\": 1}\n",
    "new_df[\"response_num\"] = new_df[\"response\"].replace(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c01654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2x2 plot figure\n",
    "fig=plt.figure(figsize=(20,10))\n",
    "plt.suptitle('Activity 4 Plot Overview', fontsize=18)\n",
    "\n",
    "#Plot 1 (0,0) for Task 1\n",
    "plt.subplot(3,3,1)\n",
    "y=cat_df[\"response\"].value_counts()\n",
    "x=y.index\n",
    "plt.bar(x,y, color = [\"red\", \"green\"])\n",
    "plt.title('Task 1: Total Number of Responses', fontsize=12, fontweight = \"bold\")\n",
    "plt.xlabel(\"count\")\n",
    "plt.ylabel(\"response\")\n",
    "\n",
    "#Plot 2 (0,1) for Task 2\n",
    "plt.subplot(3,3,2)\n",
    "plt.title('Task 2: Response by Sales Channel', fontsize=12, fontweight = \"bold\")\n",
    "sale_by_channel = new_df.groupby([\"sales_channel\", \"response\"])[\"response_num\"].count().reset_index()\n",
    "sns.barplot(x = 'sales_channel',y = 'response_num', hue = 'response', data = sale_by_channel, palette=[\"red\", \"green\"])\n",
    "\n",
    "#Plot 3 (1,0) for Task 3\n",
    "plt.subplot(3,3,3)\n",
    "plt.title('Task 3: Response by Total Claim Amount', fontsize=12, fontweight = \"bold\")\n",
    "sns.histplot(x=\"total_claim_amount\", hue=\"response\", data = new_df, palette=[\"red\", \"green\"])\n",
    "\n",
    "#Plot 4 (1,1) for Task 4\n",
    "plt.subplot(3,3,4)\n",
    "plt.title('Task 4: Response by Income', fontsize=12, fontweight = \"bold\")\n",
    "sns.histplot(x=\"income\", hue=\"response\", data = new_df, palette=[\"red\", \"green\"])\n",
    "\n",
    "plt.subplot(3,3,5)\n",
    "plt.title('Task 4: Response by Area', fontsize=12, fontweight = \"bold\")\n",
    "sns.histplot(x=\"region\", hue=\"response\", data = new_df, palette=[\"red\", \"green\"])\n",
    "\n",
    "\n",
    "plt.subplot(3,3,6)\n",
    "plt.title('Task 4: Response by Education', fontsize=12, fontweight = \"bold\")\n",
    "sns.histplot(x=\"education\", hue=\"response\", data = new_df, palette=[\"red\", \"green\"])\n",
    "\n",
    "plt.subplot(3,3,6)\n",
    "plt.title('Task 4: Total Claim Amount by Education', fontsize=12, fontweight = \"bold\")\n",
    "sns.histplot(x=\"education\", hue=\"total_claim_amount\", data = new_df, palette=[\"red\", \"green\"])\n",
    "\n",
    "\n",
    "#Adjusting layout to avoid overlap\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2bed8",
   "metadata": {},
   "source": [
    "### Task 5: (Optional) plot any interesting findings/insights(minimum three) that describe some interesting facts about your data set and its input variables as well as relationships with the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ba02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (30,20))\n",
    "for i, column in enumerate(cat_df.columns, 1):\n",
    "    plt.subplot(5,5,i)\n",
    "    plt.title(f\"{column}\")\n",
    "    sns.countplot(x=cat_df[column], data=cat_df)\n",
    "    \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_response_survival = new_df.groupby(['policy_type','response'])['response_num'].count().reset_index()\n",
    "sns.barplot(x = policy_response_survival['policy_type'],y = policy_response_survival['response_num'], hue = policy_response_survival['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_response_survival = new_df.groupby(['vehicle_size','response'])['response_num'].count().reset_index()\n",
    "sns.barplot(x = policy_response_survival['vehicle_size'],y = policy_response_survival['response_num'], hue = policy_response_survival['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd66d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (30,20))\n",
    "for i, column in enumerate(num_df.columns, 1):\n",
    "    plt.subplot(2,4,i)\n",
    "    plt.title(f\"{column}\")\n",
    "    #sns.countplot(x=cat_df[column], data=cat_df)\n",
    "    sns.scatterplot(x=new_df[\"monthly_premium_auto\"], y=new_df[\"customer_lifetime_value\"], hue=new_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c85b3b",
   "metadata": {},
   "source": [
    "## Week 4 - Monday: Activity 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd926a2a",
   "metadata": {},
   "source": [
    "### Task 1: check if there are highly correlated features and drop them if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184b1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing neccessary Python libraries (e.g. Pandas, Numpy, Matplotlib, Seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298aaade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>customer_lifetime_value</th>\n",
       "      <th>response</th>\n",
       "      <th>coverage</th>\n",
       "      <th>education</th>\n",
       "      <th>effective_to_date</th>\n",
       "      <th>month</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>...</th>\n",
       "      <th>months_since_policy_inception</th>\n",
       "      <th>number_of_open_complaints</th>\n",
       "      <th>number_of_policies</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>policy</th>\n",
       "      <th>renew_offer_type</th>\n",
       "      <th>sales_channel</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>vehicle_class</th>\n",
       "      <th>vehicle_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>central</td>\n",
       "      <td>4809</td>\n",
       "      <td>no</td>\n",
       "      <td>basic</td>\n",
       "      <td>college</td>\n",
       "      <td>2/18/11</td>\n",
       "      <td>feb</td>\n",
       "      <td>employed</td>\n",
       "      <td>m</td>\n",
       "      <td>48029</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>corporate auto</td>\n",
       "      <td>corporate l3</td>\n",
       "      <td>offer3</td>\n",
       "      <td>agent</td>\n",
       "      <td>292</td>\n",
       "      <td>four-door car</td>\n",
       "      <td>medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>west region</td>\n",
       "      <td>2228</td>\n",
       "      <td>no</td>\n",
       "      <td>basic</td>\n",
       "      <td>college</td>\n",
       "      <td>1/18/11</td>\n",
       "      <td>jan</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>f</td>\n",
       "      <td>92260</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>personal auto</td>\n",
       "      <td>personal l3</td>\n",
       "      <td>offer4</td>\n",
       "      <td>call center</td>\n",
       "      <td>744</td>\n",
       "      <td>four-door car</td>\n",
       "      <td>medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>east</td>\n",
       "      <td>14947</td>\n",
       "      <td>no</td>\n",
       "      <td>basic</td>\n",
       "      <td>bachelor</td>\n",
       "      <td>2/10/11</td>\n",
       "      <td>feb</td>\n",
       "      <td>employed</td>\n",
       "      <td>m</td>\n",
       "      <td>22139</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>personal auto</td>\n",
       "      <td>personal l3</td>\n",
       "      <td>offer3</td>\n",
       "      <td>call center</td>\n",
       "      <td>480</td>\n",
       "      <td>suv</td>\n",
       "      <td>medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>north west</td>\n",
       "      <td>22332</td>\n",
       "      <td>yes</td>\n",
       "      <td>extended</td>\n",
       "      <td>college</td>\n",
       "      <td>1/11/11</td>\n",
       "      <td>jan</td>\n",
       "      <td>employed</td>\n",
       "      <td>m</td>\n",
       "      <td>49078</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>corporate auto</td>\n",
       "      <td>corporate l3</td>\n",
       "      <td>offer2</td>\n",
       "      <td>branch</td>\n",
       "      <td>484</td>\n",
       "      <td>four-door car</td>\n",
       "      <td>medsize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>north west</td>\n",
       "      <td>9025</td>\n",
       "      <td>no</td>\n",
       "      <td>premium</td>\n",
       "      <td>bachelor</td>\n",
       "      <td>1/17/11</td>\n",
       "      <td>jan</td>\n",
       "      <td>medical leave</td>\n",
       "      <td>f</td>\n",
       "      <td>23675</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>personal auto</td>\n",
       "      <td>personal l2</td>\n",
       "      <td>offer1</td>\n",
       "      <td>branch</td>\n",
       "      <td>707</td>\n",
       "      <td>four-door car</td>\n",
       "      <td>medsize</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        region  customer_lifetime_value response  coverage education  \\\n",
       "0      central                     4809       no     basic   college   \n",
       "1  west region                     2228       no     basic   college   \n",
       "2         east                    14947       no     basic  bachelor   \n",
       "3   north west                    22332      yes  extended   college   \n",
       "4   north west                     9025       no   premium  bachelor   \n",
       "\n",
       "  effective_to_date month employment_status gender  income  ...  \\\n",
       "0           2/18/11   feb          employed      m   48029  ...   \n",
       "1           1/18/11   jan        unemployed      f   92260  ...   \n",
       "2           2/10/11   feb          employed      m   22139  ...   \n",
       "3           1/11/11   jan          employed      m   49078  ...   \n",
       "4           1/17/11   jan     medical leave      f   23675  ...   \n",
       "\n",
       "  months_since_policy_inception number_of_open_complaints  number_of_policies  \\\n",
       "0                            52                         0                   9   \n",
       "1                            26                         0                   1   \n",
       "2                            31                         0                   2   \n",
       "3                             3                         0                   2   \n",
       "4                            31                         0                   7   \n",
       "\n",
       "      policy_type        policy  renew_offer_type  sales_channel  \\\n",
       "0  corporate auto  corporate l3            offer3          agent   \n",
       "1   personal auto   personal l3            offer4    call center   \n",
       "2   personal auto   personal l3            offer3    call center   \n",
       "3  corporate auto  corporate l3            offer2         branch   \n",
       "4   personal auto   personal l2            offer1         branch   \n",
       "\n",
       "  total_claim_amount  vehicle_class vehicle_size  \n",
       "0                292  four-door car      medsize  \n",
       "1                744  four-door car      medsize  \n",
       "2                480            suv      medsize  \n",
       "3                484  four-door car      medsize  \n",
       "4                707  four-door car      medsize  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Previewing original data\n",
    "df_5 = pd.read_csv(\"/Users/lukasbauerschmidt/Desktop/Ironhack/2. Class Materials/IH_DA_FT_JAN_2023/Class_Materials/Case_Studies/Customer_Analysis_Case_Study/Data/Data_Marketing_Customer_Analysis_Round3.csv\")\n",
    "df_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d3bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_num = df_5.select_dtypes(include=['int', 'float'])\n",
    "df_5_cat = df_5.select_dtypes(exclude=['int', 'float'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48764872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_lifetime_value</th>\n",
       "      <th>income</th>\n",
       "      <th>monthly_premium_auto</th>\n",
       "      <th>months_since_last_claim</th>\n",
       "      <th>months_since_policy_inception</th>\n",
       "      <th>number_of_open_complaints</th>\n",
       "      <th>number_of_policies</th>\n",
       "      <th>total_claim_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>customer_lifetime_value</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.404235</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>0.017055</td>\n",
       "      <td>-0.032371</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.232849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>0.003732</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>-0.026564</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.018903</td>\n",
       "      <td>-0.004461</td>\n",
       "      <td>-0.111221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monthly_premium_auto</th>\n",
       "      <td>0.404235</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>-0.008811</td>\n",
       "      <td>-0.018805</td>\n",
       "      <td>0.630149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>months_since_last_claim</th>\n",
       "      <td>0.011912</td>\n",
       "      <td>-0.026564</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034086</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.013874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>months_since_policy_inception</th>\n",
       "      <td>0.017055</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.019696</td>\n",
       "      <td>-0.034086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>-0.009719</td>\n",
       "      <td>0.004186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_open_complaints</th>\n",
       "      <td>-0.032371</td>\n",
       "      <td>0.018903</td>\n",
       "      <td>-0.008811</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>-0.010668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_policies</th>\n",
       "      <td>0.016789</td>\n",
       "      <td>-0.004461</td>\n",
       "      <td>-0.018805</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>-0.009719</td>\n",
       "      <td>-0.000303</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_claim_amount</th>\n",
       "      <td>0.232849</td>\n",
       "      <td>-0.111221</td>\n",
       "      <td>0.630149</td>\n",
       "      <td>0.013874</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>-0.010668</td>\n",
       "      <td>-0.008019</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               customer_lifetime_value    income  \\\n",
       "customer_lifetime_value                       1.000000  0.003732   \n",
       "income                                        0.003732  1.000000   \n",
       "monthly_premium_auto                          0.404235 -0.002431   \n",
       "months_since_last_claim                       0.011912 -0.026564   \n",
       "months_since_policy_inception                 0.017055  0.003846   \n",
       "number_of_open_complaints                    -0.032371  0.018903   \n",
       "number_of_policies                            0.016789 -0.004461   \n",
       "total_claim_amount                            0.232849 -0.111221   \n",
       "\n",
       "                               monthly_premium_auto  months_since_last_claim  \\\n",
       "customer_lifetime_value                    0.404235                 0.011912   \n",
       "income                                    -0.002431                -0.026564   \n",
       "monthly_premium_auto                       1.000000                 0.010036   \n",
       "months_since_last_claim                    0.010036                 1.000000   \n",
       "months_since_policy_inception              0.019696                -0.034086   \n",
       "number_of_open_complaints                 -0.008811                 0.001204   \n",
       "number_of_policies                        -0.018805                 0.000814   \n",
       "total_claim_amount                         0.630149                 0.013874   \n",
       "\n",
       "                               months_since_policy_inception  \\\n",
       "customer_lifetime_value                             0.017055   \n",
       "income                                              0.003846   \n",
       "monthly_premium_auto                                0.019696   \n",
       "months_since_last_claim                            -0.034086   \n",
       "months_since_policy_inception                       1.000000   \n",
       "number_of_open_complaints                           0.002554   \n",
       "number_of_policies                                 -0.009719   \n",
       "total_claim_amount                                  0.004186   \n",
       "\n",
       "                               number_of_open_complaints  number_of_policies  \\\n",
       "customer_lifetime_value                        -0.032371            0.016789   \n",
       "income                                          0.018903           -0.004461   \n",
       "monthly_premium_auto                           -0.008811           -0.018805   \n",
       "months_since_last_claim                         0.001204            0.000814   \n",
       "months_since_policy_inception                   0.002554           -0.009719   \n",
       "number_of_open_complaints                       1.000000           -0.000303   \n",
       "number_of_policies                             -0.000303            1.000000   \n",
       "total_claim_amount                             -0.010668           -0.008019   \n",
       "\n",
       "                               total_claim_amount  \n",
       "customer_lifetime_value                  0.232849  \n",
       "income                                  -0.111221  \n",
       "monthly_premium_auto                     0.630149  \n",
       "months_since_last_claim                  0.013874  \n",
       "months_since_policy_inception            0.004186  \n",
       "number_of_open_complaints               -0.010668  \n",
       "number_of_policies                      -0.008019  \n",
       "total_claim_amount                       1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Built correlation matrix of numerical dataframe (w/o target variable)\n",
    "corr = df_5_num.corr()\n",
    "corr\n",
    "\n",
    "#monthly premium auto feature seems to be the only feature which shows \"higher\" correlation. Hence we drop this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_num.drop(\"monthly_premium_auto\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa46f91",
   "metadata": {},
   "source": [
    "### Task 2: One Hot/Label Encoding of the categorical variables in the categoricals data frame that you created in Activity 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning dataset and distinguishing between which categorical columns are nominal and ordinal\n",
    "df_5_cat.head()\n",
    "\n",
    "#drop the effective_to_date column since it is indeed numerical\n",
    "df_5_cat.drop(\"effective_to_date\", axis=1)\n",
    "nominal = [\"region\", \"response\", \"employment_status\", \"gender\", \"location_code\", \"marital_status\", \"policy_type\", \"policy\", \"renew_offer_type\", \"sales_channel\", \"vehicle_class\"]\n",
    "ordinals = [\"coverage\", \"education\", \"month\", \"vehicle_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005622cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical - nominal columns\n",
    "import sklearn\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, drop = \"first\")\n",
    "encoder_vars_array = one_hot_encoder.fit_transform(df_5_cat[nominal])\n",
    "encoder_feature_names = one_hot_encoder.get_feature_names_out(nominal)\n",
    "encoder_vars_df = pd.DataFrame(encoder_vars_array, columns = encoder_feature_names)\n",
    "encoder_vars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5[ordinals].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3dd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5[\"vehicle_size\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87110d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_cat[ordinals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical - ordinal columns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "maplist = {'coverage': {'basic': 0, 'extended': 1, 'premium': 2},\n",
    "           'education': {'high school or below': 0, 'bachelor': 1, 'college': 2, 'master': 3, 'doctor': 4},\n",
    "           'month': {'jan': 0, 'feb': 1},\n",
    "           'vehicle_size': {'small': 0, 'medsize': 1, 'large': 2,}}\n",
    "df_5_cat_ordinary_encoded = df_5_cat[ordinals].apply(lambda x: x.map(maplist[x.name]))\n",
    "df_5_cat_ordinary_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a74d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5_cat_ordinary_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ee104",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16406d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating two previously encoded/generated dataframes\n",
    "df_5_cat_encoded_total = pd.concat([df_5_cat_ordinary_encoded.reset_index(drop=True), encoder_vars_df.reset_index(drop=True)], axis = 1)\n",
    "df_5_cat_encoded_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761730ee",
   "metadata": {},
   "source": [
    "### Optional Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1077c1",
   "metadata": {},
   "source": [
    "### Task 1: Variable A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e058713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking which column shows widest range\n",
    "#Income variable has largest range\n",
    "(df_5_num.max() - df_5_num.min()).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "varA = df_5_num[\"income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d722ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking which column shows largst skewness\n",
    "#customer_lifetime_value variable has largest skew\n",
    "(df_5_num.max() - df_5_num.mean()).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa34ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "varB = df_5_num[\"customer_lifetime_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before transformation: varA distribution\n",
    "sns.histplot(varA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "varA_reshaped = varA.to_numpy().reshape(-1,1)\n",
    "minmaxscaler = MinMaxScaler()\n",
    "varA_transformed = minmaxscaler.fit_transform(varA_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##after transformation: varA distribution\n",
    "sns.histplot(varA_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a50d2a",
   "metadata": {},
   "source": [
    "### Task 2: Variable B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before transformation: varB distribution\n",
    "sns.histplot(varB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "varB_rehaped = varB.to_numpy().reshape(-1,1)\n",
    "varB_rehaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc02576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#powertransformer = PowerTransformer(method='yeo-johnson')\n",
    "powertransformer = PowerTransformer(method='box-cox')\n",
    "varB_power_transformed = powertransformer.fit_transform(varB_rehaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after power transformation: varB distribution\n",
    "sns.histplot(varB_power_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b928e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(varB_power_transformed.mean(), varB_power_transformed.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after standard scaler: varB distribution\n",
    "standardscaler = StandardScaler()\n",
    "varB_standard_scaled = standardscaler.fit_transform(varB_rehaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de05cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(varB_standard_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f63d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(varB_standard_scaled.mean(), varB_standard_scaled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19ab19",
   "metadata": {},
   "source": [
    "## Weeky 4 - Tuesday: Activity 6¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e739b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import of necessary libraries\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafefee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only assume numerical data to be revelant (for now leave out the categorical)\n",
    "df_5_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72604823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into x and y \n",
    "x = df_5_num.drop(\"total_claim_amount\", axis=1)\n",
    "y = df_5_num[\"total_claim_amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2d504a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20022aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing the dataset\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36047636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([230.08062389, 559.06281849, 340.97224399, ..., 490.61253546,\n",
       "       463.72545559, 590.35490144])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply linear regression\n",
    "#Scikit Learn Method\n",
    "model = LinearRegression()\n",
    "model.fit(x_train_scaled, y_train)\n",
    "model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfb7dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.44374050e+00 -3.35677864e+01  1.88742534e+02  1.27912773e-01\n",
      " -3.00331832e+00  2.86638652e-01  2.40707525e+00] 432.7827154718746\n"
     ]
    }
   ],
   "source": [
    "#Scikitlearn Model Interpretation\n",
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply linear regression\n",
    "#OLS Method\n",
    "X_train_const = sm.add_constant(X_train.to_numpy())\n",
    "X_test_const = sm.add_constant(X_testto_numpy())\n",
    "\n",
    "model = sm.OLS(X_train_const, y_train).fit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
